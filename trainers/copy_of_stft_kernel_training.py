# -*- coding: utf-8 -*-
"""Copy of STFT Kernel Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpARtgS80Ctmp2l1JmwvIgFSJSQu2rRg
"""

"""# Helper function"""

from nnAudio import Spectrogram
import numpy as np
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
from tqdm import tqdm

from sklearn.model_selection import train_test_split
import torch
import torchvision

from torchsummary import summary

import os
import sys
sys.path.insert(0, 'main/models/conv')
from modules.main.global_helpers import visualize_spec_bis

# Creating h
class Model(torch.nn.Module):

    def __init__(self):
        super(Model,self).__init__()
        self.stft = Spectrogram.STFT(n_fft=64, hop_length=512, trainable=True, output_format="Magnitude")
        self.CNN_layer1 = torch.nn.Conv2d(1,1, kernel_size=(4,4))
        self.CNN_layer2 = torch.nn.Conv2d(1,1, kernel_size=(4,4))
        
        self.regressor = torch.nn.Linear(27*12,1).cuda(device)

    def forward(self, x, return_spec=False):
        # print("forward")
        # print(x.shape)
        x = self.stft(x)
        if return_spec:
            return x
        # print(x.shape)
        # print(x.unsqueeze(1).shape)
        # print(x.shape)
        x = self.CNN_layer1(x.unsqueeze(1)) # unsqueeze has the purpose of adding a channel dim
        # x = self.CNN_layer1(x)
        # x = self.CNN_layer1(x)
        x = self.CNN_layer2(x)
        # print(x.shape)
        # print(x.data.size()[0])
        x = x.view(x.data.size()[0], 27*12)
        x = self.regressor(torch.relu(x))
        return torch.sigmoid(x)

def get_dummy_dataset():
    fs = 44100
    interval = 0.2
    t = np.linspace(0, interval, int(fs*interval))
    fmin = 200

    X = np.zeros(((fs//2-fmin)//2*10, int(fs*interval)), np.float32)
    Y = np.zeros((fs//2-fmin)//2*10, np.float32)
    counter=0
    for k in tqdm(range(fmin,fs//2, 2)): # Creating signals with different frequencies
         for phi in np.arange(0,1,0.25):
            X[counter]=np.sin(2*np.pi*(k*t+phi))
            Y[counter]=k/interval
            counter+=1
    return X, Y

os.environ['CUDA_VISIBLE_DEVICES']='0'

device = "cpu"
if torch.cuda.is_available():
    device = "cuda:0"

X, Y = get_dummy_dataset() # This dummy dataset generate a pure sine wave as X and frequency as the label Y
print(np.max(X[0]))
print(np.min(X[0]))
exit()

X_train, X_test, Y_train, Y_test = train_test_split(X,Y.reshape(-1,1), test_size=0.2, random_state=101)
# X_train = X_train[:200]
# Y_train = Y_train[:200]
print("length of train dataset: {}".format(len(X_train)))
print("length of test dataset: {}".format(len(X_test)))

trainset = torch.utils.data.TensorDataset(torch.tensor(X_train, device='cpu'), torch.tensor(Y_train, device='cpu'))
testset = torch.utils.data.TensorDataset(torch.tensor(X_test, device='cpu'), torch.tensor(Y_test, device='cpu'))

batch_size = 512

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)

model = Model()
model = model.to(device)
summary(model, (1, 8820))

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
loss_function = torch.nn.MSELoss()

"""# Visualizing Original Weights"""

original_basis_real = model.stft.wcos.cpu().detach().numpy().squeeze(1)
original_basis_imag = model.stft.wsin.cpu().detach().numpy().squeeze(1)
# plt.imshow(model.stft.wsin.cpu().detach().numpy().squeeze(1), aspect='auto', origin='lower')
fig = plt.figure(figsize=(20, 10))
plt.imshow(model.stft.wsin.cpu().detach().numpy().squeeze(1), aspect='auto', origin='lower')
plt.savefig("temp/his_weights_orig")
plt.close()

"""# Training Model"""

loss_train = []
loss_test = []

print("epoch\ttrain loss\ttest loss")
old_wcos = model.stft.wcos.cpu()
for e in range(40):
    loss_train_e = 0.0
    loss_test_e = 0.0

    for i, (x, y) in enumerate(trainloader):
        x = x.cuda()
        y = y.cuda()/110240 # Normalizing to labels to [0,1]
        optimizer.zero_grad()
        y_pred = model(x)
        loss = loss_function(y_pred, y)
        loss.backward() # Calculating gradient
        optimizer.step() # Updating paramenters by gradient
        loss_train_e += loss.item()
        # print(f"training {i}/{len(trainloader)} batches, loss = {loss.item():.6f}", end = '\r')
    
    loss_train.append(loss_train_e/len(trainloader))
    print("Average loss per batch (size {}): {} ".format(batch_size, loss_train[-1]))
   
    new_wcos = model.stft.wcos.cpu()
    diff = torch.sum(abs(new_wcos - old_wcos))
    print("diff of wcos: {}".format(diff))
    old_wcos = new_wcos

    with torch.no_grad():
        counter = 0
        for x, y in testloader:
            if counter == 0 and e % 10 == 0:
                index = 5
                print(y[index])
                spec_to_track = x[index].to(device)
                label_to_track = x[index]
                outputs = model(spec_to_track, return_spec=True)
                outputs = np.squeeze(outputs.to("cpu").numpy())
                # print("spec")
                # print(outputs)
                visualize_spec_bis(outputs, 8000, "temp/test", title="epoch_{}".format(e))
                counter += 1
            x = x.cuda()
            y = y.cuda()/110240
            y_pred = model(x)
            loss = loss_function(y_pred, y)
            loss_test_e +=loss.item()
        loss_test.append(loss_test_e/len(testloader))
        # print(' '*100, end='\r')
        # print(f"{e}\t{loss_train[-1]:.6f}\t{loss_test[-1]:.6f}")

"""# Visualizing Trained Weights"""

trained_basis_real = model.stft.wsin.cpu().detach().numpy().squeeze(1)
trained_basis_imag = model.stft.wcos.cpu().detach().numpy().squeeze(1)
# plt.imshow(model.stft.wsin.cpu().detach().numpy().squeeze(1), aspect='auto', origin='lower')
fig = plt.figure(figsize=(20, 10))
plt.imshow(model.stft.wsin.cpu().detach().numpy().squeeze(1), aspect='auto', origin='lower')
plt.savefig("temp/his_weights_trained")
plt.close()

"""# Have a closer look on the STFT basis"""

fig, ax = plt.subplots(5,2, figsize=(12,18))
cols = ['Original Fourier Kernels', 'Trained Fourier Kernels']
rows = np.arange(1,6)
for ax_idx, col in zip(ax[0], cols):
    ax_idx.set_title(col, size=16)
for ax_idx, row in zip(ax[:,0], rows):
    ax_idx.set_ylabel(f'k={row}', size=16)    

for i in range(1):
    ax[i,0].plot(original_basis_real[i+1], 'b')
    ax[i,1].plot(trained_basis_real[i+1], 'b')
    ax[i,0].tick_params(labelsize=12)
    ax[i,1].tick_params(labelsize=12)
    
for i in range(5):
    ax[i,0].plot(original_basis_imag[i*2+1], 'g')
    ax[i,1].plot(trained_basis_imag[i*2+1], 'g')
    ax[i,0].tick_params(labelsize=12)
    ax[i,1].tick_params(labelsize=12)
    ax[i,1].legend(['real','imaginary'])

plt.savefig("temp/his_kernels")
plt.close()

"""# Trained STFT"""
fig = plt.figure(figsize=(20, 10))
plt.imshow(model.stft(x)[0].cpu().detach().numpy(), aspect='auto', origin='lower')
plt.tick_params(labelsize=12)
plt.title('Trained STFT', size=18)
plt.savefig("temp/trained_stft")
plt.close()

"""# Original STFT"""
fig = plt.figure(figsize=(20, 10))
ori_stft = Spectrogram.STFT(n_fft=64, hop_length=512, output_format="Magnitude", trainable=True).to(device)
plt.imshow(ori_stft(x)[0].cpu().detach().numpy(), aspect='auto', origin='lower')
plt.tick_params(labelsize=12)
plt.title('Original STFT', size=18)
plt.savefig("temp/original_stft")
plt.close()

