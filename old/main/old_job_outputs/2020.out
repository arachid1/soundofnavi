Category:  coch
Dataset:  all_sw_coch_preprocessed_v2_param_v26_augm_v0_cleaned_8000.pkl
File:  conv.train2020
Description:  test_MoE
2021-03-25 17:45:30.815794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 17:45:39.776210: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 17:45:39.804243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 17:45:40.473375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:40.474225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-03-25 17:45:40.476305: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 17:45:40.540309: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 17:45:40.540425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 17:45:40.568118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 17:45:40.594958: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 17:45:40.611919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 17:45:40.634092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 17:45:40.640431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 17:45:40.640636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:40.641542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:40.643822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 17:45:40.649217: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 17:45:40.649407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:40.650225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-03-25 17:45:40.650257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 17:45:40.650295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 17:45:40.650310: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 17:45:40.650323: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 17:45:40.650336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 17:45:40.650358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 17:45:40.650373: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 17:45:40.650386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 17:45:40.650465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:40.651220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:40.651884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 17:45:40.653193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 17:45:42.702812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 17:45:42.702857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 17:45:42.702866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 17:45:42.706387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:42.707290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:42.708050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 17:45:42.708703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14760 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
2021-03-25 17:45:43.337420: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-03-25 17:45:43.337469: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-03-25 17:45:43.338730: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2021-03-25 17:45:43.339009: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcupti.so.11.0'; dlerror: libcupti.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64
2021-03-25 17:45:43.376929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so
2021-03-25 17:45:43.584548: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-03-25 17:45:43.587226: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
Tensorflow Version: 2.4.0
Num GPUs Available:  1
-----------------------
Using module located at /main/models/conv/train2020.py
Using train_file located at ../../data/datasets/all_sw_coch_preprocessed_v2_param_v26_augm_v0_cleaned_8000.pkl
Using logs_path located at ../../cache/conv___03_1744___all_sw_coch_preprocessed_v2_param_v26_augm_v0_cleaned_8000___test_MoE___2020/logs/
-----------------------
Collecting Variables...
Model Parameters: {'N_CLASSES': 2, 'SR': 8000, 'BATCH_SIZE': 24, 'LR': 0.001, 'SHAPE': (128, 1250, 3), 'WEIGHT_DECAY': 0.0001, 'LL2_REG': 0, 'EPSILON': 1e-07, 'LABEL_SMOOTHING': 0}
Learning Rate Parameters: {'factor': 0.5, 'patience': 3, 'min_lr': 1e-05}
Early Stopping Patience and Delta: 8, 1.0%
-----------------------
Size of training set: 13945
Size of validation set: 3486
Initializing weights...
weights = {0: 0.6399045521292217, 1: 1.2145345596432553, 2: 1.1234209847898944, 3: 1.3816582117945466}
Model: "conv2d"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 128, 1250, 3 0                                            
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 128, 1250, 3) 12          input_1[0][0]                    
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 128, 1250, 16 64          batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 1250, 16 64          batch_normalization[0][0]        
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 1250, 3) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 128, 1250, 16 2320        conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 1250, 16 6416        conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 1250, 16 64          max_pooling2d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 128, 1250, 48 0           conv2d_1[0][0]                   
                                                                 conv2d_3[0][0]                   
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 64, 625, 48)  0           concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 64, 625, 48)  192         average_pooling2d[0][0]          
__________________________________________________________________________________________________
dropout (Dropout)               (None, 64, 625, 48)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
inverted_residual (InvertedResi (None, 64, 625, 32)  35840       dropout[0][0]                    
__________________________________________________________________________________________________
inverted_residual_1 (InvertedRe (None, 64, 625, 32)  20864       inverted_residual[0][0]          
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 32, 312, 32)  0           inverted_residual_1[0][0]        
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 312, 32)  128         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 312, 32)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
inverted_residual_2 (InvertedRe (None, 32, 312, 64)  27136       dropout_1[0][0]                  
__________________________________________________________________________________________________
inverted_residual_3 (InvertedRe (None, 32, 312, 64)  66304       inverted_residual_2[0][0]        
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 16, 156, 64)  0           inverted_residual_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 16, 156, 64)  256         average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 16, 156, 64)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
inverted_residual_4 (InvertedRe (None, 16, 156, 128) 91136       dropout_2[0][0]                  
__________________________________________________________________________________________________
inverted_residual_5 (InvertedRe (None, 16, 156, 128) 230912      inverted_residual_4[0][0]        
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 8, 78, 128)   0           inverted_residual_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 8, 78, 128)   512         average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 8, 78, 128)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
inverted_residual_6 (InvertedRe (None, 8, 78, 256)   329728      dropout_3[0][0]                  
__________________________________________________________________________________________________
inverted_residual_7 (InvertedRe (None, 8, 78, 256)   855040      inverted_residual_6[0][0]        
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 4, 39, 256)   0           inverted_residual_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 4, 39, 256)   1024        average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 4, 39, 256)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
inverted_residual_8 (InvertedRe (None, 4, 39, 512)   1249280     dropout_4[0][0]                  
__________________________________________________________________________________________________
inverted_residual_9 (InvertedRe (None, 4, 39, 512)   3282944     inverted_residual_8[0][0]        
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 512)          0           inverted_residual_9[0][0]        2021-03-25 17:45:44.755584: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 17:45:44.763218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz
2021-03-25 17:45:48.732891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 17:45:50.999886: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 17:45:51.046095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 17:45:59.529910: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-03-25 17:45:59.529953: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-03-25 17:46:01.180651: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-03-25 17:46:01.184438: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-03-25 17:46:01.228284: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 2396 callback api events and 2392 activity events. 
2021-03-25 17:46:01.286618: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.

__________________________________________________________________________________________________
dense_mo_e (DenseMoE)           (None, 512)          526338      global_average_pooling2d[0][0]   
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            1026        dense_mo_e[0][0]                 
==================================================================================================
Total params: 6,727,600
Trainable params: 6,686,090
Non-trainable params: 41,510
__________________________________________________________________________________________________
Epoch 1/40
582/582 - 699s - loss: 0.3588 - calc_accuracy: 0.6036 - val_loss: 0.4465 - val_calc_accuracy: 0.6347
2021-03-25 17:59:10.938195: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Epoch 2/40
582/582 - 679s - loss: 0.2670 - calc_accuracy: 0.7187 - val_loss: 0.7240 - val_calc_accuracy: 0.5494
2021-03-25 18:13:28.435605: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-03-25 18:18:31.099993: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Epoch 3/40
582/582 - 682s - loss: 0.2382 - calc_accuracy: 0.7466 - val_loss: 0.6541 - val_calc_accuracy: 0.6467
Epoch 4/40
582/582 - 710s - loss: 0.2330 - calc_accuracy: 0.7547 - val_loss: 0.3239 - val_calc_accuracy: 0.7520
Epoch 5/40
582/582 - 703s - loss: 0.2185 - calc_accuracy: 0.7708 - val_loss: 0.3197 - val_calc_accuracy: 0.7432
2021-03-25 18:46:28.173416: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Epoch 6/40
582/582 - 709s - loss: 0.2078 - calc_accuracy: 0.7867 - val_loss: 0.3703 - val_calc_accuracy: 0.7206
Epoch 7/40
582/582 - 723s - loss: 0.1796 - calc_accuracy: 0.8197 - val_loss: 0.2659 - val_calc_accuracy: 0.8099
Epoch 8/40
582/582 - 737s - loss: 0.1670 - calc_accuracy: 0.8296 - val_loss: 0.2590 - val_calc_accuracy: 0.8199
Epoch 9/40
582/582 - 727s - loss: 0.1598 - calc_accuracy: 0.8389 - val_loss: 0.2299 - val_calc_accuracy: 0.8293
Epoch 10/40
582/582 - 700s - loss: 0.1512 - calc_accuracy: 0.8472 - val_loss: 0.1976 - val_calc_accuracy: 0.8570
2021-03-25 19:48:22.918939: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 703.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Epoch 11/40
582/582 - 699s - loss: 0.1434 - calc_accuracy: 0.8564 - val_loss: 0.3181 - val_calc_accuracy: 0.7814
Epoch 12/40
582/582 - 713s - loss: 0.1389 - calc_accuracy: 0.8586 - val_loss: 0.1774 - val_calc_accuracy: 0.8739
Epoch 13/40
582/582 - 731s - loss: 0.1332 - calc_accuracy: 0.8653 - val_loss: 0.1783 - val_calc_accuracy: 0.8773
2021-03-25 20:25:14.904997: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-03-25 20:28:19.356897: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 703.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Epoch 14/40
582/582 - 728s - loss: 0.1244 - calc_accuracy: 0.8767 - val_loss: 0.2138 - val_calc_accuracy: 0.8584
2021-03-25 20:33:39.936699: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-03-25 20:33:41.107523: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-03-25 20:41:14.644763: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.03GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Epoch 15/40
582/582 - 724s - loss: 0.1228 - calc_accuracy: 0.8766 - val_loss: 0.1801 - val_calc_accuracy: 0.8744
Epoch 16/40
582/582 - 712s - loss: 0.1161 - calc_accuracy: 0.8840 - val_loss: 0.1862 - val_calc_accuracy: 0.8647

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 17/40
582/582 - 702s - loss: 0.0993 - calc_accuracy: 0.8982 - val_loss: 0.1667 - val_calc_accuracy: 0.8821
Epoch 18/40
582/582 - 702s - loss: 0.0914 - calc_accuracy: 0.9051 - val_loss: 0.2424 - val_calc_accuracy: 0.8467
Epoch 19/40
582/582 - 717s - loss: 0.0866 - calc_accuracy: 0.9122 - val_loss: 0.1585 - val_calc_accuracy: 0.8981
Epoch 20/40
582/582 - 723s - loss: 0.0840 - calc_accuracy: 0.9134 - val_loss: 0.1644 - val_calc_accuracy: 0.8930
Epoch 21/40
582/582 - 725s - loss: 0.0796 - calc_accuracy: 0.9150 - val_loss: 0.1496 - val_calc_accuracy: 0.8987
Epoch 22/40
582/582 - 717s - loss: 0.0770 - calc_accuracy: 0.9207 - val_loss: 0.1456 - val_calc_accuracy: 0.9055
Epoch 23/40
582/582 - 703s - loss: 0.0711 - calc_accuracy: 0.9255 - val_loss: 0.2259 - val_calc_accuracy: 0.8662
Epoch 24/40
582/582 - 700s - loss: 0.0666 - calc_accuracy: 0.9283 - val_loss: 0.1593 - val_calc_accuracy: 0.9024
Epoch 25/40
582/582 - 710s - loss: 0.0636 - calc_accuracy: 0.9346 - val_loss: 0.2027 - val_calc_accuracy: 0.8799

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 26/40
582/582 - 725s - loss: 0.0512 - calc_accuracy: 0.9475 - val_loss: 0.1500 - val_calc_accuracy: 0.9181
Epoch 27/40
582/582 - 731s - loss: 0.0479 - calc_accuracy: 0.9473 - val_loss: 0.1541 - val_calc_accuracy: 0.9098
Epoch 28/40
582/582 - 725s - loss: 0.0442 - calc_accuracy: 0.9523 - val_loss: 0.1544 - val_calc_accuracy: 0.9115
Epoch 29/40
582/582 - 715s - loss: 0.0407 - calc_accuracy: 0.9562 - val_loss: 0.2260 - val_calc_accuracy: 0.8927

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 30/40
582/582 - 700s - loss: 0.0358 - calc_accuracy: 0.9621 - val_loss: 0.1571 - val_calc_accuracy: 0.9167
Epoch 31/40
582/582 - 697s - loss: 0.0348 - calc_accuracy: 0.9622 - val_loss: 0.1923 - val_calc_accuracy: 0.9084
Epoch 32/40
582/582 - 706s - loss: 0.0336 - calc_accuracy: 0.9658 - val_loss: 0.1579 - val_calc_accuracy: 0.9167

Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 33/40
582/582 - 720s - loss: 0.0294 - calc_accuracy: 0.9688 - val_loss: 0.1599 - val_calc_accuracy: 0.9184
Epoch 34/40
582/582 - 733s - loss: 0.0287 - calc_accuracy: 0.9692 - val_loss: 0.1698 - val_calc_accuracy: 0.9209
Epoch 00034: early stopping

 See logs at ../../cache/conv___03_1744___all_sw_coch_preprocessed_v2_param_v26_augm_v0_cleaned_8000___test_MoE___2020/logs/