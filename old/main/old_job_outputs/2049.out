Category:  coch
Dataset:  all_sw_coch_preprocessed_v2_param_v29_augm_v0_cleaned_8000.pkl
File:  conv.train2049
Description:  test
2021-04-13 01:21:27.318439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 01:21:29.319667: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 01:21:29.320641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-13 01:21:29.372819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:29.373531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-04-13 01:21:29.373559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 01:21:29.377015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 01:21:29.377093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 01:21:29.378218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 01:21:29.378522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 01:21:29.379552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 01:21:29.380427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 01:21:29.380598: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 01:21:29.380707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:29.381435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:29.382140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 01:21:29.384429: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-13 01:21:29.384614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:29.385309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-04-13 01:21:29.385336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 01:21:29.385365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 01:21:29.385380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 01:21:29.385394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-13 01:21:29.385408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-13 01:21:29.385434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-13 01:21:29.385448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-13 01:21:29.385462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 01:21:29.385529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:29.386208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:29.386900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-13 01:21:29.386936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-13 01:21:30.100107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-13 01:21:30.100153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-13 01:21:30.100162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-13 01:21:30.100444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:30.101244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:30.101946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-13 01:21:30.102611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14760 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0)
2021-04-13 01:21:30.466895: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-04-13 01:21:30.466932: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-04-13 01:21:30.466967: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2021-04-13 01:21:30.467163: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcupti.so.11.0'; dlerror: libcupti.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64
2021-04-13 01:21:30.468050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so
2021-04-13 01:21:30.661448: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-04-13 01:21:30.664045: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-04-13 01:21:31.184185: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-13 01:21:31.185814: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz
2021-04-13 01:21:32.488587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-13 01:21:32.891461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-13 01:21:32.898648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-13 01:21:35.586307: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-04-13 01:21:35.586346: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-04-13 01:21:36.197276: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-04-13 01:21:36.200808: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-04-13 01:21:36.253889: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 1452 callback api events and 1448 activity events. 
2021-04-13 01:21:36.290582: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0532s vs `on_train_batch_begin` time: 0.0909s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0532s vs `on_train_batch_end` time: 0.0577s). Check your callbacks.
Tensorflow Version: 2.4.0
Num GPUs Available:  1
-----------------------
Using module located at /main/models/conv/train2049.py
Using train_file located at ../../data/datasets/all_sw_coch_preprocessed_v2_param_v29_augm_v0_cleaned_8000.pkl
Using logs_path located at ../../cache/conv___04_0121___all_sw_coch_preprocessed_v2_param_v29_augm_v0_cleaned_8000___test___2049/logs/
-----------------------
Collecting Variables...
Model Parameters: {'N_CLASSES': 1, 'SR': 8000, 'BATCH_SIZE': 16, 'LR': 0.001, 'SHAPE': (128, 1250, 3), 'WEIGHT_DECAY': 0.0001, 'LL2_REG': 0, 'EPSILON': 1e-07, 'LABEL_SMOOTHING': 0}
Learning Rate Parameters: {'factor': 0.75, 'patience': 4, 'min_lr': 1e-05}
Early Stopping Patience and Delta: 15, 1.0%
-----------------------
Size of training set: 4316
Size of validation set: 1078
Initializing weights...
weights = {0: 0.6776381909547738, 1: 1.9073550212164074}
Model: "conv2d"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 128, 1250, 3)]    0         
_________________________________________________________________
batch_normalization (BatchNo (None, 128, 1250, 3)      12        
_________________________________________________________________
conv2d (Conv2D)              (None, 128, 1250, 4)      16        
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 1250, 8)      1160      
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 128, 1250, 16)     144       
_________________________________________________________________
average_pooling2d (AveragePo (None, 64, 625, 16)       0         
_________________________________________________________________
dropout (Dropout)            (None, 64, 625, 16)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 625, 16)       64        
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 64, 625, 16)       9232      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 64, 625, 32)       544       
_________________________________________________________________
average_pooling2d_1 (Average (None, 32, 313, 32)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 313, 32)       0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 313, 32)       128       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 32, 313, 32)       36896     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 32, 313, 64)       2112      
_________________________________________________________________
average_pooling2d_2 (Average (None, 16, 157, 64)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 157, 64)       0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 157, 64)       256       
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 16, 157, 64)       147520    
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 16, 157, 128)      8320      
_________________________________________________________________
average_pooling2d_3 (Average (None, 8, 79, 128)        0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 79, 128)        0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 8, 79, 128)        512       
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 8, 79, 128)        589952    
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 8, 79, 256)        33024     
_________________________________________________________________
average_pooling2d_4 (Average (None, 4, 40, 256)        0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 4, 40, 256)        0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 4, 40, 256)        1024      
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 4, 40, 256)        65792     
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 4, 40, 512)        1180160   
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 4, 40, 512)        2359808   
_________________________________________________________________
global_average_pooling2d (Gl (None, 512)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 513       
=================================================================
Total params: 4,437,189
Trainable params: 4,436,191
Non-trainable params: 998
_________________________________________________________________
Epoch 1/40
270/270 - 88s - loss: 0.7043 - accuracy: 0.4750 - val_loss: 0.6903 - val_accuracy: 0.7282
Epoch 2/40
270/270 - 83s - loss: 0.6887 - accuracy: 0.6515 - val_loss: 0.6922 - val_accuracy: 0.5668
Epoch 3/40
270/270 - 87s - loss: 0.6926 - accuracy: 0.6601 - val_loss: 0.6816 - val_accuracy: 0.7254
Epoch 4/40
270/270 - 84s - loss: 0.6860 - accuracy: 0.6666 - val_loss: 0.6831 - val_accuracy: 0.6929
Epoch 5/40
270/270 - 102s - loss: 0.6865 - accuracy: 0.6272 - val_loss: 0.6795 - val_accuracy: 0.6920

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.
Epoch 6/40
270/270 - 107s - loss: 0.6811 - accuracy: 0.6260 - val_loss: 0.7305 - val_accuracy: 0.4471
Epoch 7/40
270/270 - 108s - loss: 0.6816 - accuracy: 0.6309 - val_loss: 0.6956 - val_accuracy: 0.5649
Epoch 8/40
270/270 - 106s - loss: 0.6802 - accuracy: 0.6351 - val_loss: 0.6751 - val_accuracy: 0.6707
Epoch 9/40
270/270 - 108s - loss: 0.6768 - accuracy: 0.6675 - val_loss: 0.7323 - val_accuracy: 0.6466

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.
Epoch 10/40
270/270 - 105s - loss: 0.6751 - accuracy: 0.6478 - val_loss: 0.6817 - val_accuracy: 0.6011
Epoch 11/40
270/270 - 109s - loss: 0.6722 - accuracy: 0.6443 - val_loss: 0.6588 - val_accuracy: 0.7032
Epoch 12/40
270/270 - 107s - loss: 0.6670 - accuracy: 0.6520 - val_loss: 0.6852 - val_accuracy: 0.6596
Epoch 13/40
270/270 - 109s - loss: 0.6642 - accuracy: 0.6506 - val_loss: 0.6532 - val_accuracy: 0.6939

Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.
Epoch 14/40
270/270 - 106s - loss: 0.6546 - accuracy: 0.6786 - val_loss: 0.6318 - val_accuracy: 0.7171
Epoch 15/40
270/270 - 108s - loss: 0.6486 - accuracy: 0.6867 - val_loss: 1.0594 - val_accuracy: 0.2913
Epoch 16/40
270/270 - 105s - loss: 0.6377 - accuracy: 0.6849 - val_loss: 0.7228 - val_accuracy: 0.5325
2021-04-13 01:48:45.064005: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-04-13 01:48:45.064049: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-04-13 01:48:46.129012: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-04-13 01:48:46.133273: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-04-13 01:48:46.199521: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 1419 callback api events and 1415 activity events. 
2021-04-13 01:48:46.242301: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.5620s). Check your callbacks.
Epoch 00016: early stopping
Transitioning to the other model (lower lr and more regularization) 
Model: "other"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 128, 1250, 3 0                                            
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 1250, 3) 12          input_2[0][0]                    
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 1250, 16 64          batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 128, 1250, 16 64          batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 1250, 3) 0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 1250, 16 2320        conv2d_14[0][0]                  
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 128, 1250, 16 6416        conv2d_16[0][0]                  
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 128, 1250, 16 64          max_pooling2d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 128, 1250, 48 0           conv2d_15[0][0]                  
                                                                 conv2d_17[0][0]                  
                                                                 conv2d_18[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 64, 625, 48)  0           concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 625, 48)  192         average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 64, 625, 48)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
inverted_residual (InvertedResi (None, 64, 625, 32)  35840       dropout_5[0][0]                  
__________________________________________________________________________________________________
inverted_residual_1 (InvertedRe (None, 64, 625, 32)  20864       inverted_residual[0][0]          
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 32, 312, 32)  0           inverted_residual_1[0][0]        
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 312, 32)  128         average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 32, 312, 32)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
inverted_residual_2 (InvertedRe (None, 32, 312, 64)  27136       dropout_6[0][0]                  
__________________________________________________________________________________________________
inverted_residual_3 (InvertedRe (None, 32, 312, 64)  66304       inverted_residual_2[0][0]        
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 16, 156, 64)  0           inverted_residual_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 156, 64)  256         average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 156, 64)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
inverted_residual_4 (InvertedRe (None, 16, 156, 128) 91136       dropout_7[0][0]                  
__________________________________________________________________________________________________
inverted_residual_5 (InvertedRe (None, 16, 156, 128) 230912      inverted_residual_4[0][0]        
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 78, 128)   0           inverted_residual_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 8, 78, 128)   512         average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 8, 78, 128)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
inverted_residual_6 (InvertedRe (None, 8, 78, 256)   329728      dropout_8[0][0]                  
__________________________________________________________________________________________________
inverted_residual_7 (InvertedRe (None, 8, 78, 256)   855040      inverted_residual_6[0][0]        
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 4, 39, 256)   0           inverted_residual_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 4, 39, 256)   1024        average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 4, 39, 256)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
inverted_residual_8 (InvertedRe (None, 4, 39, 512)   1249280     dropout_9[0][0]                  
__________________________________________________________________________________________________
inverted_residual_9 (InvertedRe (None, 4, 39, 512)   3282944     inverted_residual_8[0][0]        
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 512)          0           inverted_residual_9[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            513         global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 6,200,749
Trainable params: 6,159,239
Non-trainable params: 41,510
__________________________________________________________________________________________________
Epoch 1/40
270/270 - 217s - loss: 0.7300 - accuracy: 0.5422 - val_loss: 0.6764 - val_accuracy: 0.7254
Epoch 2/40
270/270 - 209s - loss: 0.6915 - accuracy: 0.5832 - val_loss: 0.7227 - val_accuracy: 0.2746
Epoch 3/40
270/270 - 210s - loss: 0.6814 - accuracy: 0.5575 - val_loss: 0.5935 - val_accuracy: 0.7263
Epoch 4/40
270/270 - 209s - loss: 0.6631 - accuracy: 0.6050 - val_loss: 0.6393 - val_accuracy: 0.6391
Epoch 5/40
270/270 - 209s - loss: 0.6600 - accuracy: 0.5987 - val_loss: 0.5938 - val_accuracy: 0.7124
Epoch 6/40
270/270 - 209s - loss: 0.6506 - accuracy: 0.6165 - val_loss: 0.6856 - val_accuracy: 0.5798
Epoch 7/40
270/270 - 209s - loss: 0.6382 - accuracy: 0.6138 - val_loss: 0.6206 - val_accuracy: 0.6976

Epoch 00007: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-05.
Epoch 8/40
270/270 - 209s - loss: 0.6145 - accuracy: 0.6487 - val_loss: 0.8998 - val_accuracy: 0.4592
Epoch 9/40
270/270 - 209s - loss: 0.6002 - accuracy: 0.6661 - val_loss: 0.6148 - val_accuracy: 0.7199
Epoch 10/40
270/270 - 208s - loss: 0.5798 - accuracy: 0.6796 - val_loss: 0.7510 - val_accuracy: 0.5427
Epoch 11/40
270/270 - 210s - loss: 0.5688 - accuracy: 0.6930 - val_loss: 0.7408 - val_accuracy: 0.5649

Epoch 00011: ReduceLROnPlateau reducing learning rate to 5.6249997214763425e-05.
Epoch 12/40
270/270 - 209s - loss: 0.5159 - accuracy: 0.7305 - val_loss: 0.6052 - val_accuracy: 0.7152
Epoch 13/40
270/270 - 209s - loss: 0.4883 - accuracy: 0.7373 - val_loss: 0.6765 - val_accuracy: 0.6289
Epoch 14/40
270/270 - 209s - loss: 0.4485 - accuracy: 0.7641 - val_loss: 0.9332 - val_accuracy: 0.5863
Epoch 15/40
270/270 - 212s - loss: 0.4306 - accuracy: 0.7813 - val_loss: 0.7591 - val_accuracy: 0.5733

Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.218749927531462e-05.
Epoch 16/40
270/270 - 208s - loss: 0.3988 - accuracy: 0.7982 - val_loss: 1.2253 - val_accuracy: 0.4647
2021-04-13 02:44:49.608671: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-04-13 02:44:49.608749: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-04-13 02:44:50.729327: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-04-13 02:44:50.733828: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-04-13 02:44:50.778230: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 1421 callback api events and 1417 activity events. 
2021-04-13 02:44:50.818531: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.6292s). Check your callbacks.
Epoch 00016: early stopping
Last with label smoothing.
Model: "other"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 128, 1250, 3 0                                            
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 128, 1250, 3) 12          input_3[0][0]                    
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 128, 1250, 16 64          batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 128, 1250, 16 64          batch_normalization_12[0][0]     
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 1250, 3) 0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 128, 1250, 16 2320        conv2d_19[0][0]                  
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 128, 1250, 16 6416        conv2d_21[0][0]                  
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 128, 1250, 16 64          max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 1250, 48 0           conv2d_20[0][0]                  
                                                                 conv2d_22[0][0]                  
                                                                 conv2d_23[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 64, 625, 48)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 64, 625, 48)  192         average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 64, 625, 48)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
inverted_residual_10 (InvertedR (None, 64, 625, 32)  35840       dropout_10[0][0]                 
__________________________________________________________________________________________________
inverted_residual_11 (InvertedR (None, 64, 625, 32)  20864       inverted_residual_10[0][0]       
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 32, 312, 32)  0           inverted_residual_11[0][0]       
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 32, 312, 32)  128         average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 32, 312, 32)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
inverted_residual_12 (InvertedR (None, 32, 312, 64)  27136       dropout_11[0][0]                 
__________________________________________________________________________________________________
inverted_residual_13 (InvertedR (None, 32, 312, 64)  66304       inverted_residual_12[0][0]       
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 16, 156, 64)  0           inverted_residual_13[0][0]       
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 156, 64)  256         average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 16, 156, 64)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
inverted_residual_14 (InvertedR (None, 16, 156, 128) 91136       dropout_12[0][0]                 
__________________________________________________________________________________________________
inverted_residual_15 (InvertedR (None, 16, 156, 128) 230912      inverted_residual_14[0][0]       
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 8, 78, 128)   0           inverted_residual_15[0][0]       
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 8, 78, 128)   512         average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 8, 78, 128)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
inverted_residual_16 (InvertedR (None, 8, 78, 256)   329728      dropout_13[0][0]                 
__________________________________________________________________________________________________
inverted_residual_17 (InvertedR (None, 8, 78, 256)   855040      inverted_residual_16[0][0]       
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 4, 39, 256)   0           inverted_residual_17[0][0]       
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 4, 39, 256)   1024        average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 4, 39, 256)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
inverted_residual_18 (InvertedR (None, 4, 39, 512)   1249280     dropout_14[0][0]                 
__________________________________________________________________________________________________
inverted_residual_19 (InvertedR (None, 4, 39, 512)   3282944     inverted_residual_18[0][0]       
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 512)          0           inverted_residual_19[0][0]       
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 6,200,749
Trainable params: 6,159,239
Non-trainable params: 41,510
__________________________________________________________________________________________________
Epoch 1/40
270/270 - 216s - loss: 0.7295 - accuracy: 0.5317 - val_loss: 0.6997 - val_accuracy: 0.2746
Epoch 2/40
270/270 - 213s - loss: 0.6939 - accuracy: 0.5677 - val_loss: 0.7015 - val_accuracy: 0.2755
Epoch 3/40
270/270 - 213s - loss: 0.6875 - accuracy: 0.5843 - val_loss: 0.6718 - val_accuracy: 0.7254
Epoch 4/40
270/270 - 212s - loss: 0.6869 - accuracy: 0.5843 - val_loss: 0.6694 - val_accuracy: 0.7078
Epoch 5/40
270/270 - 212s - loss: 0.6781 - accuracy: 0.6033 - val_loss: 0.6709 - val_accuracy: 0.6596
Epoch 6/40
270/270 - 212s - loss: 0.6773 - accuracy: 0.6223 - val_loss: 0.6716 - val_accuracy: 0.6920
Epoch 7/40
270/270 - 213s - loss: 0.6731 - accuracy: 0.6103 - val_loss: 0.6576 - val_accuracy: 0.7161

Epoch 00007: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-05.
Epoch 8/40
270/270 - 215s - loss: 0.6662 - accuracy: 0.6404 - val_loss: 0.7026 - val_accuracy: 0.5965
Epoch 9/40
270/270 - 213s - loss: 0.6598 - accuracy: 0.6842 - val_loss: 0.6773 - val_accuracy: 0.6967
Epoch 10/40
270/270 - 213s - loss: 0.6574 - accuracy: 0.6777 - val_loss: 0.6770 - val_accuracy: 0.6577
Epoch 11/40
270/270 - 213s - loss: 0.6520 - accuracy: 0.6949 - val_loss: 0.6607 - val_accuracy: 0.6967

Epoch 00011: ReduceLROnPlateau reducing learning rate to 5.6249997214763425e-05.
Epoch 12/40
270/270 - 212s - loss: 0.6389 - accuracy: 0.7076 - val_loss: 0.6753 - val_accuracy: 0.6939
Epoch 13/40
270/270 - 212s - loss: 0.6302 - accuracy: 0.7396 - val_loss: 0.6576 - val_accuracy: 0.7319
Epoch 14/40
270/270 - 212s - loss: 0.6242 - accuracy: 0.7572 - val_loss: 0.6603 - val_accuracy: 0.7440
Epoch 15/40
270/270 - 213s - loss: 0.6152 - accuracy: 0.7806 - val_loss: 0.6476 - val_accuracy: 0.7523
Epoch 16/40
270/270 - 210s - loss: 0.6099 - accuracy: 0.7968 - val_loss: 0.6726 - val_accuracy: 0.6902
Epoch 17/40
270/270 - 212s - loss: 0.6021 - accuracy: 0.8146 - val_loss: 0.6433 - val_accuracy: 0.7607
Epoch 18/40
270/270 - 212s - loss: 0.5933 - accuracy: 0.8299 - val_loss: 0.6786 - val_accuracy: 0.7189
Epoch 19/40
270/270 - 212s - loss: 0.5892 - accuracy: 0.8443 - val_loss: 0.6912 - val_accuracy: 0.7607
Epoch 20/40
270/270 - 213s - loss: 0.5850 - accuracy: 0.8503 - val_loss: 0.6765 - val_accuracy: 0.7597
Epoch 21/40
270/270 - 206s - loss: 0.5794 - accuracy: 0.8747 - val_loss: 0.6688 - val_accuracy: 0.7403

Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.218749927531462e-05.
Epoch 22/40
270/270 - 206s - loss: 0.5742 - accuracy: 0.8809 - val_loss: 0.6816 - val_accuracy: 0.6781
Epoch 23/40
270/270 - 207s - loss: 0.5696 - accuracy: 0.8902 - val_loss: 0.7236 - val_accuracy: 0.6243
Epoch 24/40
270/270 - 208s - loss: 0.5677 - accuracy: 0.8948 - val_loss: 0.6658 - val_accuracy: 0.7449
Epoch 25/40
270/270 - 207s - loss: 0.5624 - accuracy: 0.9133 - val_loss: 0.6638 - val_accuracy: 0.7597

Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.164062582072802e-05.
Epoch 26/40
270/270 - 206s - loss: 0.5613 - accuracy: 0.9078 - val_loss: 0.6701 - val_accuracy: 0.7690
Epoch 27/40
270/270 - 205s - loss: 0.5615 - accuracy: 0.9092 - val_loss: 0.6594 - val_accuracy: 0.7273
Epoch 28/40
270/270 - 206s - loss: 0.5589 - accuracy: 0.9136 - val_loss: 0.6580 - val_accuracy: 0.7644
Epoch 29/40
270/270 - 207s - loss: 0.5574 - accuracy: 0.9194 - val_loss: 0.6533 - val_accuracy: 0.7625
Epoch 30/40
270/270 - 206s - loss: 0.5573 - accuracy: 0.9226 - val_loss: 0.6782 - val_accuracy: 0.7644

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.3730469365546014e-05.
Epoch 31/40
270/270 - 209s - loss: 0.5556 - accuracy: 0.9224 - val_loss: 0.6524 - val_accuracy: 0.7727
Epoch 32/40
270/270 - 207s - loss: 0.5519 - accuracy: 0.9377 - val_loss: 0.6583 - val_accuracy: 0.7690
Epoch 33/40
270/270 - 208s - loss: 0.5529 - accuracy: 0.9307 - val_loss: 0.6569 - val_accuracy: 0.7699
Epoch 34/40
270/270 - 206s - loss: 0.5518 - accuracy: 0.9316 - val_loss: 0.6598 - val_accuracy: 0.7495
Epoch 35/40
270/270 - 207s - loss: 0.5542 - accuracy: 0.9314 - val_loss: 0.6594 - val_accuracy: 0.7737
Epoch 36/40
270/270 - 207s - loss: 0.5503 - accuracy: 0.9421 - val_loss: 0.6590 - val_accuracy: 0.7579
Epoch 37/40
270/270 - 206s - loss: 0.5518 - accuracy: 0.9321 - val_loss: 0.6555 - val_accuracy: 0.7254
Epoch 38/40
270/270 - 206s - loss: 0.5499 - accuracy: 0.9407 - val_loss: 0.6540 - val_accuracy: 0.7468
Epoch 39/40
270/270 - 206s - loss: 0.5486 - accuracy: 0.9472 - val_loss: 0.6509 - val_accuracy: 0.7607

Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.7797852706280537e-05.
Epoch 40/40
270/270 - 207s - loss: 0.5488 - accuracy: 0.9435 - val_loss: 0.6511 - val_accuracy: 0.7727

 See logs at ../../cache/conv___04_0121___all_sw_coch_preprocessed_v2_param_v29_augm_v0_cleaned_8000___test___2049/logs/